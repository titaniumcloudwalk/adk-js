/**
 * @license
 * Copyright 2025 Google LLC
 * SPDX-License-Identifier: Apache-2.0
 */

import {Content, FunctionDeclaration, GenerateContentConfig, LiveConnectConfig, SchemaUnion} from '@google/genai';

import {BaseTool} from '../tools/base_tool.js';
import {CacheMetadata, ContextCacheConfig} from './cache_metadata.js';

/**
 * LLM request class that allows passing in tools, output schema and system
 * instructions to the model.
 */
export interface LlmRequest {
  /**
   * The model name.
   */
  model?: string;

  /**
   * The contents to send to the model.
   */
  contents: Content[];

  /**
   * Additional config for the generate content request.
   * Tools in generateContentConfig should not be set directly; use appendTools.
   */
  config?: GenerateContentConfig;

  liveConnectConfig: LiveConnectConfig;

  /**
   * The tools dictionary. Excluded from JSON serialization.
   */
  toolsDict: {[key: string]: BaseTool};

  /**
   * Configuration for context caching behavior.
   *
   * When set, controls how context caching is used to optimize costs
   * and performance for frequently used content.
   */
  cacheConfig?: ContextCacheConfig;

  /**
   * Metadata for an existing context cache.
   *
   * If provided, the LLM will attempt to use this cached content
   * instead of reprocessing the full context.
   */
  cacheMetadata?: CacheMetadata;

  /**
   * The number of tokens in the cacheable contents.
   *
   * This is used to determine if caching is beneficial based on
   * the minimum token threshold in cacheConfig.
   */
  cacheableContentsTokenCount?: number;

  /**
   * Previous interaction ID for Gemini Interactions API.
   *
   * When using the Interactions API, this field contains the ID from
   * the previous interaction, enabling stateful conversation without
   * sending full history.
   */
  previousInteractionId?: string;
}

/**
 * Appends instructions to the system instruction.
 * @param instructions The instructions to append.
 */
export function appendInstructions(
    llmRequest: LlmRequest,
    instructions: string[],
    ): void {
  if (!llmRequest.config) {
    llmRequest.config = {};
  }
  const newInstructions = instructions.join('\n\n');
  if (llmRequest.config.systemInstruction) {
    llmRequest.config.systemInstruction += '\n\n' + newInstructions;
  } else {
    llmRequest.config.systemInstruction = newInstructions;
  }
}

  /**
   * Appends tools to the request.
   * @param tools The tools to append.
   */
export function appendTools(
    llmRequest: LlmRequest,
    tools: BaseTool[],
    ): void {
  if (!tools?.length) {
    return;
  }

  const functionDeclarations: FunctionDeclaration[] = [];
  for (const tool of tools) {
    const declaration = tool._getDeclaration();
    if (declaration) {
      functionDeclarations.push(declaration);
      llmRequest.toolsDict[tool.name] = tool;
    }
  }

  if (functionDeclarations.length) {
    if (!llmRequest.config) {
      llmRequest.config = {};
    }
    if (!llmRequest.config.tools) {
      llmRequest.config.tools = [];
    }
    llmRequest.config.tools.push({functionDeclarations});
  }
}

  /**
   * Sets the output schema for the request.
   *
   * @param schema The JSON Schema object to set as the output schema.
   */
export function setOutputSchema(
    llmRequest: LlmRequest,
    schema: SchemaUnion,
    ): void {
  if (!llmRequest.config) {
    llmRequest.config = {};
  }
  llmRequest.config.responseSchema = schema;
  llmRequest.config.responseMimeType = 'application/json';
}
